{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparable-capture",
   "metadata": {},
   "source": [
    "## Task 3 - NLP Tasks\n",
    "### NLP Task 1: Text Summarizer\n",
    "#### Literature Review\n",
    "Extraction based text summarisation can be completed through a number of approaches. One such system, TextRank, proposed in Sehgal et al (2017) modifies Google’s PageRank algorithm to extract useful and important phrases from the available text. This paper evaluates the algorithm through the ROGUE 2.0 Evaluation toolkit and achieves the following results:\n",
    "\n",
    "| Rogue Type | Task Name | Average Recall | Average Precision | Average Fscore | Number Referance Summaries |\n",
    "|------------|-----------|----------------|-------------------|----------------|----------------------------|\n",
    "| ROGUE 1    | Sample 1  | 1.0            | 0.29664           | 0.45732        | 1                          |\n",
    "| ROGUE 1    | Sample 2  | 1.0            | 0.09125           | 0.16841        | 1                          |\n",
    "| ROGUE 1    | Sample 3  | 1.0            | 0.33504           | 0.50192        | 1                          |\n",
    "| ROGUE 1    | Sample 4  | 1.0            | 0.44071           | 0.61180        | 1                          |\n",
    "\n",
    "While efforts had been made to extract a meaningful and coherent summary\n",
    "from the article, there is still a lot of scope of improvement as to how the sentences are extracted and whether they take the summary to its logical meaning. Another possible approach is to use k-means clustering as proposed in Shetty and Kallimani (2017). This paper proposes a three-step unsupervised extraction approach consisting of:\n",
    "-\tDocument tokenization\n",
    "-\tCompute sentence score\n",
    "-\tApply Centroid Based Clustering on the sentences and extract important sentences as part of summary\n",
    "\n",
    "The paper does not measure the accuracy of the summarizer and instead compares their output to a human written abstractive summary. They state that their approach provides more favourable results than current state-of-the-art approaches such as TextRank, ranking their proposed approach in second place to a human approach, very close to the first place.\n",
    "\n",
    "#### Rational for selection of the NLP task\n",
    "With the recent surge in the amount of online content available to the general population, a fast and effective automatic summarization has become more important. By summarizing content with a large amount of data down to a few sentences, users are able to access the most important aspects of the content without having to sift through redundant and insignificant information. This proves key in reducing the amount of time taken reading job applications, allowing the prospective applicant to read a quick summarization of the post without needing to read the whole thing.\n",
    "\n",
    "An extraction-based approach was chosen as opposed to an abstractive-based summariser due to the simplicity of the former method. The extraction-based approach also retains the writing style and nuances of the original job description which can provide context that would be lost with an abstractive text summariser. \n",
    "\n",
    "#### Data pre-processing of inputs and outputs, separate from the WebCrawler harvesting\n",
    "The input to the summarizer will be the job description, with the output being an arbitrary amount of the most impactful sentences as determined by the extractor. For this use case, three sentences were determined to be satisfactory. As the job description had been cleaned previously, there is no additional pre-processing of the input required. As the output is just text sentences extracted from the input, is requires no additional cleaning or processing before the summary can be stored alongside the rest of the data in a csv file.\n",
    "\n",
    "#### Specification and justification of hyperparameter\n",
    "As this NLP task is not a machine learning task, no hyperparameter is used. Judging the effectiveness of the summarizer will be down to pure human judgement as this task is only a prototype. Further research can be done into what metrics can be applied and how to apply them to this task.\n",
    "\n",
    "#### Preliminary assessment of NLP Task performance\n",
    "To test the summarizer, we can first test it on a single job ad to gauge its effectiveness. Further to that we can then run the summarizer for each job in order to find the average length of the summary compared to the job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas nltk progressbar2 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pursuant-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (8866 of 8866) |####################| Elapsed Time: 0:01:43 Time:  0:01:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rail design manager job in melbourne \n",
      "========================================================================================================================================================================================================\n",
      "Original description:\n",
      "just imagine your future with us… at aurecon we see the future through a very different lens. do you? innovation, eminence and digital are at the heart of everything we do. are you excited about the future? are you driven by the opportunity to work on some of the most challenging and complex projects around the world and to learn from the best? we are. diversity is at the core of everything we do. we work together to create a culture based on respect, trust and inclusiveness. our differences are what fuel our creativity. we embrace flexible working and are always open to discussing your individual needs so that you get to create your own experience with us. what will you do? our rail infrastructure team support the technical advisory & design delivery of large-scale major projects across victoria. we are seeking rail design managers to join our rail team working on several high-profile, city-shaping projects including suburban rail loop, geelong fast rail, melbourne airport link, level crossing removal program, regional rail revitalisation and other upcoming projects in both the rail planning and delivery phases. some of the things you will do to bring ideas to life: lead/coordinate a multi-disciplinary design team, including design partners, in rail planning and/or delivery projects (light rail or heavy rail). work closely with our clients to listen, understand, and then address their needs manage resources, subcontractors, and costs to deliver the projects produce reports and other deliverables as required be actively involved in safety in design, risk assessments assist the resolution of design standard or operational requirements non-conformances assist with the preparation of proposals for future work what can you bring to the team? we are looking for dedicated rail design management professionals to join our award-winning team to leave a legacy on victorias rail industry. you will have a proven background working on rail projects with a development and/or delivery focus. you will come to us with working knowledge of the multiple disciplines that make up a rail project. an understanding of the department of transport planning process and mtm, v/line, yarra trams, artc design process requirements will be highly regarded. finally, we value that each of our team members brings something different to aurecon. we look for people who have had a broad range of experiences throughout their career and can demonstrate how they have worked as part of a team to bring ideas to life. does that sound like you? about us weve re-imagined engineering. aurecon is an engineering and infrastructure advisory company, but not as you know it! for a start, our clients ideas drive what we do. drawing on our deep pool of expertise, we co-create innovative solutions with our clients to some of the worlds most complex challenges. and through a range of unique creative processes and skills, we work to re-imagine, shape and design a better future.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Summarized description:\n",
      "an understanding of the department of transport planning process and mtm, v/line, yarra trams, artc design process requirements will be highly regarded. and through a range of unique creative processes and skills, we work to re-imagine, shape and design a better future. we are looking for dedicated rail design management professionals to join our award-winning team to leave a legacy on victorias rail industry.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Done!\n",
      "Mean original length: 414.07861493345365\n",
      "Mean summarized length: 69.72355064290548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_summary(row):\n",
    "    formatted_article_text = row.DESCRIPTION\n",
    "    # Sentence wise Tokenizing\n",
    "    sentence_list = nltk.sent_tokenize(row.DESCRIPTION)\n",
    "\n",
    "    # Find Weighted Frequency of Occurrence\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "\n",
    "    # Calculating Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    # Getting the Summary\n",
    "    return heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('data/clean_seek_data.csv')\n",
    "    sum_lens = []\n",
    "\n",
    "    with progressbar.ProgressBar(max_value=len(data)) as bar:\n",
    "        for index, row in data.iterrows():\n",
    "            summary_sentences = get_summary(row)\n",
    "            summary = \" \".join(summary_sentences)\n",
    "            summary_len = len(nltk.word_tokenize(summary))\n",
    "            sum_lens.append(summary_len)\n",
    "            bar.update(index)\n",
    "\n",
    "    data = data.sample()\n",
    "    for index, row in data.iterrows():\n",
    "        summary_sentences = get_summary(row)\n",
    "        summary = \" \".join(summary_sentences)\n",
    "        print(row.TITLE)\n",
    "        print(\"=\"*200)\n",
    "        print(\"Original description:\")\n",
    "        print(row.DESCRIPTION)\n",
    "        print(\"-\"*200)\n",
    "        print(\"Summarized description:\")\n",
    "        print(summary)\n",
    "        print(\"-\" * 200)\n",
    "        print()\n",
    "        original_len = len(nltk.word_tokenize(row.DESCRIPTION))\n",
    "        summary_len = len(nltk.word_tokenize(summary))\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(\"Mean summarized length: {}\".format(np.round(np.mean(sum_lens))))\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-romance",
   "metadata": {},
   "source": [
    "To test the summarizer, we can first test it on a single job ad to gauge its effectiveness. Further to that we can then run the summarizer for each job in order to find the average length of the summary compared to the job posting.\n",
    "\n",
    "From the output, the summary lacks coherence as it is simply extracting impactful sentences and does not try to develop a new paragraph based on the data, a task more suitable to an abstractive -based summarizer. We can also see the summary has an average length of 70 words, a 453% reduction from the average 317 words in a job posting. \n",
    "\n",
    "While the summary lacks coherence, its' quality is more than suitable for this prototype task. As such the summarizer can be applied to the whole dataset with the summary saved in a column titled “SUMMARY”.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import heapq\n",
    "import progressbar\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Summarizing Job Data!\")\n",
    "    data = pd.read_csv('data/clean_seek_data.csv')\n",
    "\n",
    "    with progressbar.ProgressBar(max_value=len(data)) as bar:\n",
    "        for index, row in data.iterrows():\n",
    "            summary_sentences = get_summary(row)    \n",
    "            summary = \" \".join(summary_sentences)\n",
    "            data.loc[index, 'SUMMARY'] = summary\n",
    "            bar.update(index)\n",
    "\n",
    "    # Saving Data\n",
    "    data.to_csv(\"summarised_job_data.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-premiere",
   "metadata": {},
   "source": [
    "### References\n",
    "Sehgal, S., Kumar, B., Maheshwar, Rampal, L., & Chaliya, A. (2017). A Modification to Graph Based Approach for Extraction Based Automatic Text Summarization. Advances in Intelligent Systems and Computing, 373–378. https://doi.org/10.1007/978-981-10-6875-1_36\n",
    "\n",
    "Shetty, K., & Kallimani, J. S. (2017). Automatic extractive text summarization using K-means clustering. 2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT). Published. https://doi.org/10.1109/iceeccot.2017.8284627\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-exhibit",
   "metadata": {},
   "source": [
    "### NLP Task 2: Skill Keyword Extraction\n",
    "#### Literature Review\n",
    "Job candidates obtain skills through formal education, vocational education, internships, on-the-job training, or experience from previous occupations. The key function of a job search engine is to assist in the matching of the candidates skills to jobs that also require a similar skill set. A common approach while doing a skill match is to use standard keyword matching\n",
    "or information retrieval framework as explained in (Salton & Buckley, 1988). A few challenges with this approach include:\n",
    "-\tThe skill may be referenced in many different forms or synonyms (e.g., OOP, Object Oriented Programming)\n",
    "-\tSome skills may not be explicitly stated in the job description, but industry knowledge would dictate experience with the stated skill requires experience with the unstated skill (e.g. Experience with python denotes the candidate would require experience with Object Oriented Programming)\n",
    "-\tA skill dictionary would quickly become outdated as new skills from unseen and emerging domains appear.\n",
    "A framework for skill extraction and normalization was proposed in (Sharma, 2019). This paper proposes the use of a Recurrent Neural Network subtype of an artificial neural network in combination with word embeddings to solve the problems encountered with a static skill dictionary. The system first extracts noun phrases from job descriptions before applying a Long Short-Term Memory (LSTM) deep learning network combined with word embeddings to extract the relevant skills from the text. The authors were able to achieve a test accuracy of 76.58% by restricting the job domain to jobs Data Science category. The method proposed in this paper is limited by the use of noun phrases for the core training dataset. As many job posts are represented by verb phrases, a new training set and model must be developed to extract the phrases. \n",
    "\n",
    "#### Rational for selection of the NLP task\n",
    "Job skills are the common link between job applications, applicant resumes and job listings by companies. Identifying skills in job postings is a significant problem and can provide a pathway for job seekers and hiring organisation. By ‘tagging’ each job listing with the required skills and enabling users to filter jobs by these skills would drastically improve the job search process.\n",
    "\n",
    "#### Data pre-processing of inputs and outputs, separate from the WebCrawler harvesting\n",
    "The input to the skill extractor will be noun phrases as defined in  by the following grammar:\n",
    "\n",
    "NBAR:\n",
    "\n",
    "    {<NN.\\*|JJ>\\*<NN.\\*>} \n",
    "\n",
    "\n",
    "NP:\n",
    "\n",
    "    {\\<NBAR>}\n",
    "    {\\<NBAR>\\<IN>\\<NBAR>}\n",
    "    \n",
    "These noun phrases will be extracted from each job description and fed through the RNN with the output being a float value from 0 to 1 representing the probability that the noun phrase is a skill. If the output of the RNN is greater than 0.5, the phrase is determined to be a skill and is appended to a list of skills for that job description. This list is then saved into a column in the dataset’s csv file titled “SKILLS”.\n",
    "\n",
    "#### Specification and justification of hyperparameter\n",
    "For the specified problem, there are multiple classes (skill and not_skill) but only one of the classes can be present at a single time. As such, the softmax activation function was chosen as it enables the model to interpret the output as probabilities.\n",
    "\n",
    "The implemented RNN model also uses the adaptive moment estimation, adam, optimizer from keras which uses default values of:\n",
    "-\tLearning rate = 0.001\n",
    "-\tBeta_1 = 0.9\n",
    "-\tBeta_2 = 0.999\n",
    "-\tEpsilon = 1e-7\t\n",
    "\n",
    "Keras also offers multiple metrics to judge the model. In our case, the accuracy of the models prediction will be used in order to provide a proper comparison to past literature.\n",
    "\n",
    "#### Preliminary assessment of NLP Task performance\n",
    "Utilising an industry standard 80/20 split on the labelled data along with a 5-fold cross validation, we can judge the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metropolitan-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing the RNN\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "11/11 [==============================] - 20s 25ms/step - loss: 0.6919\n",
      "Epoch 2/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6769\n",
      "Epoch 3/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6661\n",
      "Epoch 4/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6001\n",
      "Epoch 5/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.5493\n",
      "Epoch 6/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.4149\n",
      "Epoch 7/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.3507\n",
      "The accuracy score is: 0.7153846153846154\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "11/11 [==============================] - 4s 25ms/step - loss: 0.6932\n",
      "Epoch 2/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6802\n",
      "Epoch 3/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6590\n",
      "Epoch 4/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6494\n",
      "Epoch 5/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6058\n",
      "Epoch 6/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.5169\n",
      "Epoch 7/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.3929\n",
      "The accuracy score is: 0.7538461538461538\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "11/11 [==============================] - 4s 23ms/step - loss: 0.7053\n",
      "Epoch 2/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6898\n",
      "Epoch 3/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6748\n",
      "Epoch 4/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6504\n",
      "Epoch 5/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.5658\n",
      "Epoch 6/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.4518\n",
      "Epoch 7/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.3184\n",
      "The accuracy score is: 0.7107692307692308\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "11/11 [==============================] - 4s 26ms/step - loss: 0.7139\n",
      "Epoch 2/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6869\n",
      "Epoch 3/7\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.6839\n",
      "Epoch 4/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6604\n",
      "Epoch 5/7\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.6147\n",
      "Epoch 6/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.5575\n",
      "Epoch 7/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.3934\n",
      "The accuracy score is: 0.7180277349768875\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "11/11 [==============================] - 4s 25ms/step - loss: 0.7001\n",
      "Epoch 2/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6922\n",
      "Epoch 3/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6791\n",
      "Epoch 4/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6562\n",
      "Epoch 5/7\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.6270\n",
      "Epoch 6/7\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.5321\n",
      "Epoch 7/7\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.4113\n",
      "The accuracy score is: 0.7134052388289677\n",
      "--------------------------------------------------------------------------------\n",
      "Total mean accuracy is: 0.7222865947611711\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "import progressbar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# RNN Imports\n",
    "from pipeline import Pipeline\n",
    "from text_preprocessing import TextToTensor\n",
    "from text_chunker import Chunker\n",
    "\n",
    "# Reading the configuration file\n",
    "import yaml\n",
    "\n",
    "with open(\"conf.yml\", 'r') as file:\n",
    "    conf = yaml.safe_load(file).get('pipeline')\n",
    "    \n",
    "save_results = conf.get('save_results')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Reading the data\n",
    "data = pd.read_csv('data/train.csv')[['TEXT', 'TARGET']]\n",
    "\n",
    "# Shuffling the data for the k fold analysis\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# Creating the input for the pipeline\n",
    "X = data['TEXT'].astype(str).tolist()\n",
    "Y = data['TARGET'].tolist()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "if conf.get('k_fold'):\n",
    "    print('=' * 80)\n",
    "    print('Testing the RNN')\n",
    "    print('-' * 80)\n",
    "    kfold = KFold(n_splits=conf.get('n_splits'))\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "        # Fitting the model and forecasting with a subset of data\n",
    "        k_results = Pipeline(\n",
    "            X_train=np.array(X_train)[train_index],\n",
    "            Y_train=np.array(Y_train)[train_index],\n",
    "            embed_path='embeddings\\\\glove.840B.300d.txt',\n",
    "            embed_dim=300,\n",
    "            X_test=np.array(X_train)[test_index],\n",
    "            Y_test=np.array(Y_train)[test_index],\n",
    "            max_len=conf.get('max_len'),\n",
    "            epochs=conf.get('epochs'),\n",
    "            batch_size=conf.get('batch_size')\n",
    "        )\n",
    "        # Saving the accuracy\n",
    "        acc += [k_results.acc]\n",
    "        print(f'The accuracy score is: {acc[-1]}')\n",
    "        print('-' * 80)\n",
    "    print(f'Total mean accuracy is: {np.mean(acc)}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-hollow",
   "metadata": {},
   "source": [
    "The average accuracy of the model after the cross validation is 72.23%. Whilst less accurate than models from previous literature, this model is not restricted to covering a single job industry and proves to work well enough for a prototype task.\n",
    "\n",
    "After proving sufficiently accurate, the model was used to extract the skills from each description in the seek dataset.\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print('=' * 80)\n",
    "print('Running the RNN pipeline')\n",
    "print('-' * 80)\n",
    "print()\n",
    "# Running the pipeline with all the data\n",
    "RNN = Pipeline(\n",
    "    X_train=X_train,\n",
    "    Y_train=Y_train,\n",
    "    embed_path='embeddings\\\\glove.840B.300d.txt',\n",
    "    embed_dim=300,\n",
    "    stop_words=stop_words,\n",
    "    max_len=conf.get('max_len'),\n",
    "    epochs=conf.get('epochs'),\n",
    "    batch_size=conf.get('batch_size')\n",
    ")\n",
    "RNN.model.summary();\n",
    "print()\n",
    "print(f'Finished in {time.time() - start_time}s \\n\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Extracting skills from job descriptions...\")\n",
    "filename = f'data/skills_output_{date.today()}_{time.strftime(\"%H-%M-%S\", time.localtime())}.csv'\n",
    "job_data = pd.read_csv('data/clean_seek_data.csv').iloc[:, :]\n",
    "\n",
    "TextToTensor_instance = TextToTensor(\n",
    "        tokenizer=RNN.tokenizer,\n",
    "        max_len=conf.get('max_len')\n",
    "    )\n",
    "\n",
    "TextChunker_instance = Chunker(\n",
    "        grammar=r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "if save_results:\n",
    "    with open(filename, 'a', newline='', encoding='utf-8-sig') as file:\n",
    "        w = csv.writer(file, delimiter=',')\n",
    "        fields = [\n",
    "            'FIELD',\n",
    "            'SUB-FIELD',\n",
    "            'TITLE',\n",
    "            'DESCRIPTION',\n",
    "            'SKILLS'\n",
    "        ]\n",
    "        try: \n",
    "            w.writerow(fields)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "with progressbar.ProgressBar(max_value=len(job_data)) as bar:\n",
    "    for index, row in job_data.iterrows():\n",
    "        job_desc = row['DESCRIPTION']\n",
    "        chunks = TextChunker_instance.get_continuous_chunks(job_desc)\n",
    "        skills = []\n",
    "        for chunk in chunks:\n",
    "            chunk_nn = TextToTensor_instance.string_to_tensor([\" \".join(chunk)])\n",
    "            p_chunk = RNN.model.predict(chunk_nn)[0][0]\n",
    "            if p_chunk > 0.5:\n",
    "                skills.append(\" \".join(chunk))\n",
    "        skills = list(set(skills))\n",
    "        job_data.loc[index, 'SKILLS'] = ','.join(skills)\n",
    "        bar.update(index)\n",
    "        # Saving the predictions to a csv file\n",
    "        if save_results:\n",
    "            with open(filename, 'a', newline='', encoding='utf-8-sig') as file:\n",
    "                w = csv.writer(file, delimiter=',')\n",
    "                fields = [\n",
    "                    row['FIELD'],\n",
    "                    row['SUB-FIELD'],\n",
    "                    row['TITLE'],\n",
    "                    job_desc,\n",
    "                    ','.join(skills)\n",
    "                ]\n",
    "                try:\n",
    "                    w.writerow(fields)\n",
    "                except Exception:\n",
    "                    print(\"Failed to write index {}: {} - {}\".format(index, row['TITLE'], job_desc))\n",
    "        \n",
    "        \n",
    "print()\n",
    "print(f'Finished in {time.time() - start_time}s \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-hayes",
   "metadata": {},
   "source": [
    "### References\n",
    "Salton, G. S., & Buckley, C. B. (1988). Term-weighting approaches in automatic text retrieval. Information Processing & Management, 24(5), 513–523.\n",
    "\n",
    "Sharma, N. K. (2019, September). Job skills extraction with LSTM and word embeddings. Confusedcoders. https://confusedcoders.com/wp-content/uploads/2019/09/Job-Skills-extraction-with-LSTM-and-Word-Embeddings-Nikita-Sharma.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
